{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    def forward_propagation(self, input):\n",
    "        \"\"\"Computes the output Y of a layer for a given input X\n",
    "        \n",
    "        Args:\n",
    "            input : input X to layer\n",
    "        \n",
    "        Returns:\n",
    "            output : output Y of layer\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        \"\"\"Computes dE/dX for a given dE/dY\n",
    "        \n",
    "        Update weights and bias by dE/dW and dE/dB respectively\n",
    "        \n",
    "        Args:\n",
    "            output_error : output error dE/dY of layer\n",
    "            learning_rate : learning rate of layer\n",
    "        \n",
    "        Returns:\n",
    "            input_error : input error dE/dX of layer\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer(Layer):\n",
    "    def __init__(self, input_size, output_size, weights = None, bias = None):\n",
    "        \"\"\"Initializes fully connected layer with input_size input neurons and output_size output neurons\n",
    "        \n",
    "        Args:\n",
    "            input_size : number of input neurons\n",
    "            output_size : number of output neurons\n",
    "            weights (optional) : 2D-array (input_size * output_size) of weights\n",
    "            bias (optional) : 2D-array (1 * output_size) of bias\n",
    "        \"\"\"\n",
    "        if weights is not None:\n",
    "            self.weights = weights\n",
    "            self.bias = bias\n",
    "        else:\n",
    "            self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "            self.bias = np.random.rand(1, output_size) - 0.5\n",
    "    \n",
    "    def forward_propagation(self, input_data) -> list[int]:\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "    \n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        bias_error = output_error\n",
    "        \n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * bias_error\n",
    "        \n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        \"\"\"Initializes activation layer with activation function and its derivative\n",
    "        \n",
    "        Args:\n",
    "            activation : activation function\n",
    "            activation_prime : activation function's derivative\n",
    "        \"\"\"\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "    \n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "    \n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return np.exp(-x) / (1 + np.exp(-x)) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_pred - y_true, 2))\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "    \n",
    "    def add(self, layer):\n",
    "        \"\"\"Adds layer to network\n",
    "        \n",
    "        Args:\n",
    "            layer : Network layer\n",
    "        \"\"\"\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def use(self, loss, loss_prime):\n",
    "        \"\"\"Set loss to use\n",
    "        \n",
    "        Args:\n",
    "            loss : loss function\n",
    "            loss_prime : loss function's derivative\n",
    "        \"\"\"\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "    \n",
    "    def forward_propagation(self, input_data):\n",
    "        \"\"\"Computes the output Y of network for a given input X\n",
    "        \n",
    "        Args:\n",
    "            input_data : input X to network\n",
    "        \n",
    "        Returns:\n",
    "            output_data : output Y of network\n",
    "        \"\"\"\n",
    "        output_data = input_data\n",
    "        for layer in self.layers:\n",
    "            output_data = layer.forward_propagation(output_data)\n",
    "        return output_data\n",
    "    \n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        \"\"\"Computes input error from output error of network\n",
    "        \n",
    "        Update weights and bias of network\n",
    "        \n",
    "        Args:\n",
    "            output_error : output error dE/dY of last layer\n",
    "        \n",
    "        Returns:\n",
    "            input_error: input error dE/dX of first layer\n",
    "        \"\"\"\n",
    "        input_error = output_error\n",
    "        for layer in reversed(self.layers):\n",
    "            input_error = layer.backward_propagation(input_error, learning_rate)\n",
    "        return input_error\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        \"\"\"Predict outputs for given inputs\n",
    "        \n",
    "        Args:\n",
    "            input_data : inputs\n",
    "        \n",
    "        Returns:\n",
    "            output_data : outputs\n",
    "        \"\"\"\n",
    "        return [self.forward_propagation(sample) for sample in input_data]\n",
    "    \n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        \"\"\"Train the network\n",
    "        \n",
    "        Args:\n",
    "            x_train : inputs of training dataset\n",
    "            y_train : outputs of training dataset\n",
    "            epochs : number of iterations\n",
    "            learning_rate : learning rate of network\n",
    "        \"\"\"\n",
    "        num_samples = len(x_train)\n",
    "        epoch_digits = len(str(epochs))\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            error = 0\n",
    "            \n",
    "            for j in range(num_samples):\n",
    "                output = self.forward_propagation(x_train[j])\n",
    "                error += self.loss(y_train[j], output)\n",
    "                \n",
    "                output_error = self.loss_prime(y_train[j], output)\n",
    "                self.backward_propagation(output_error, learning_rate)\n",
    "            \n",
    "            error /= num_samples\n",
    "            \n",
    "            if i == epochs - 1 or i % (epochs // 5) == 0:\n",
    "                print(f'epoch {i + 1:>{epoch_digits}}/{epochs}  error = {error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      1/100000  error = 0.29388381339376796\n",
      "epoch  20001/100000  error = 4.14772453044984e-06\n",
      "epoch  40001/100000  error = 3.4989364394887786e-07\n",
      "epoch  60001/100000  error = 4.072941040442058e-08\n",
      "epoch  80001/100000  error = 5.198883261043304e-09\n",
      "epoch 100000/100000  error = 6.847362672796639e-10\n",
      "[array([[0.01002674, 0.98997442]])]\n"
     ]
    }
   ],
   "source": [
    "net = Network()\n",
    "\n",
    "weights1 = np.array([[0.15, 0.25],\n",
    "                     [0.20, 0.30]])\n",
    "weights2 = np.array([[0.35, 0.45],\n",
    "                     [0.40, 0.50]])\n",
    "bias1 = np.array([[0.35, 0.35]])\n",
    "bias2 = np.array([[0.60, 0.60]])\n",
    "\n",
    "x_train = np.array([[[0.50, 0.10]]])\n",
    "y_train = np.array([[[0.01, 0.99]]])\n",
    "\n",
    "net.add(FCLayer(2, 2, weights1, bias1))\n",
    "net.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "net.add(FCLayer(2, 2, weights2, bias2))\n",
    "net.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, 100000, 0.1)\n",
    "\n",
    "print(net.predict([[0.50, 0.10]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
